---
title: "BCC_Prediction"
author: "Dipanta"
date: "`r Sys.Date()`"
output: pdf_document
---


# Problem Statement

We got the dataset of a U.S. bank customer for getting the information that , this particular customer will leave bank or not.
Bases upon independent feature we have to predict the customer will exited or not.


### Importing Libraries

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(corrplot)
library(cowplot)
library(caret)
library(tibble)
library(car)
library(caTools)
library(knitr)
library(e1071)
library(randomForest)
library(xgboost)


```

# Preprocessing

```{r}
df = read.csv("Churn_Modelling.csv")
```

```{r}
head(df)
```

```{r}
summary(df)
```
- There is no missing value.

- Exited is our dependent variable.

```{r}
# Calculate correlations
correlations = cor(df[, sapply(df, is.numeric) & names(df) != "Exited"], df$Exited) * 100

# Create a data frame for correlations
correlations_df = data.frame(Column = names(df)[sapply(df, is.numeric) & names(df) != "Exited"],
                              Correlation = correlations)

# Sort the data frame by correlation
correlations_df = correlations_df[order(-correlations_df$Correlation), ]

# Print the correlations table
print(correlations_df)


```

- IsActiveMember is has deep -ve correlation with a customer leaving (obvious). ie. Active/Regular customers are highily unlikely to leave.

- Age has a mild correlation with Exited. People with more age are likely to leave.

- Mild +ve correlation is observed for Balanced as well.


```{r}
# Calculate correlations
cor_matrix = cor(df[, sapply(df, is.numeric)])

# Create a correlation heatmap
corrplot(cor_matrix, method = "color", tl.col = "black", tl.srt = 45)
```


```{r}
head(df)
```

### Checking data imbalance

```{r}
# Group by Gender and calculate max credit score
max_credit_by_gender <- df %>%
  group_by(Gender) %>%
  summarize(max_credit = max(CreditScore))

print(max_credit_by_gender)
```

```{r}
# Group by Gender and HasCrCard, then count the occurrences
card_counts_by_gender <- df %>%
  group_by(Gender, HasCrCard) %>%
  summarize(count = n())

print(card_counts_by_gender)
```


```{r}
# Create a bar plot
ggplot(card_counts_by_gender, aes(x = factor(HasCrCard), y = count, fill = factor(Gender))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Has Credit Card", y = "Count", fill = "Gender") +
  scale_fill_discrete(name = "Gender") +
  theme_minimal() +
  theme(legend.position = "top")
```


```{r}
# Gender Vs Credit_score
ggplot(df, aes(x = factor(Gender), y = CreditScore)) +
  geom_boxplot(fill = "red", outlier.shape = NA) +
  labs(x = "Gender", y = "Credit Score") +
  theme_minimal()
```


```{r}
# Has_credit vs Age
ggplot(df, aes(x = factor(HasCrCard), y = Age)) +
  geom_boxplot(fill = "blue", outlier.shape = NA) +
  labs(x = "Has Credit Card", y = "Age") +
  theme_minimal()

```


### Label Encoding

```{r}
# Create label encoding dictionaries
gender_labels = c("Female" = 0, "Male" = 1)
geography_labels = c("France" = 0, "Germany" = 1, "Spain" = 2)

# Apply label encoding to the Gender column
df$Gender = gender_labels[df$Gender]

# Apply label encoding to the Geography column
df$Geography = geography_labels[df$Geography]
```

### Feature selection
```{r}
# Remove specified columns
columns_to_remove = c("RowNumber", "CustomerId", "Surname")
df = df[, !(names(df) %in% columns_to_remove)]

```

### Checking VIF

```{r}
selected_vars <- c("CreditScore", "Gender", "Age", "Tenure", "Balance", "HasCrCard", "IsActiveMember", "EstimatedSalary", "Geography")
model <- lm(df$Exited ~ ., data = df[, selected_vars])


# Calculate VIF
vif_values <- vif(model)

print(vif_values)
```


### Split The Data set

```{r}

set.seed(123)

split = sample.split(df$Exited, SplitRatio = 0.8)

training_set = df[split, ]
test_set = df[!split, ]

```

```{r}
# Feature Scaling
training_set[-11] = scale(training_set[-11])
test_set[-11] = scale(test_set[-11])
```


# Model Building

*Logistic Regression*


```{r}
# Fitting Logistic Regression to the Training set
classifier1 = glm(formula = Exited ~ .,
                 family = binomial,
                 data = training_set)

```


```{r}
# Predicting the Test set results
prob_pred = predict(classifier1, type = 'response', newdata = test_set[-11])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
```


```{r}
# Making the Confusion Matrix
conf_matrix1 = table(test_set[, 11], y_pred > 0.5)
print(conf_matrix1)

```



### Evalution matrics

```{r}

# Calculate evaluation metrics
accuracy <- sum(diag(conf_matrix1)) / sum(conf_matrix1)
precision <- conf_matrix1[2, 2] / sum(conf_matrix1[, 2])
recall <- conf_matrix1[2, 2] / sum(conf_matrix1[2, ])
f1_score <- 2 * (precision * recall) / (precision + recall)


# Create a data frame for the metrics
metrics_df <- data.frame(
  Metric = c("Precision", "Recall", "F1 Score", "Accuracy"),
  Value = c(precision, recall, f1_score, accuracy)
)

# Print the metrics table
kable(metrics_df, format = "html", caption = "Evaluation Metrics for log_reg")


```

*SVM*

```{r}
# Fitting SVM to the Training set
classifier2 = svm(formula = Exited ~ .,
                 data = training_set,
                 type = 'C-classification',
                 kernel = 'linear')

# Predicting the Test set results
y_pred = predict(classifier2, newdata = test_set[-11])

# Making the Confusion Matrix
conf_matrix2 = table(test_set[, 11], y_pred)
print(conf_matrix2)
```

*Naive_bayes*

```{r}
# Fitting SVM to the Training set

classifier3 = naiveBayes(x = training_set[-11],
                        y = training_set$Exited)

# Predicting the Test set results
y_pred = predict(classifier3, newdata = test_set[-11])

# Making the Confusion Matrix
conf_matrix3 = table(test_set[, 11], y_pred)
print(conf_matrix3)
```

### Evalution matrics

```{r}

# Calculate evaluation metrics
accuracy <- sum(diag(conf_matrix3)) / sum(conf_matrix3)
precision <- conf_matrix3[2, 2] / (conf_matrix3[2, 2] + conf_matrix3[1, 2])
recall <- conf_matrix3[2, 2] / sum(conf_matrix3[2, ])
f1_score <- 2 * (precision * recall) / (precision + recall)

# Create a data frame for the metrics
metrics_df <- data.frame(
  Metric = c("Precision", "Recall", "F1 Score", "Accuracy"),
  Value = c(precision, recall, f1_score, accuracy)
)

# Print the metrics table
kable(metrics_df, format = "html", caption = "Evaluation Metrics for Naive_Baiyes")

```



*XGBoost*

```{r}
# Fitting XGBoost to the Training set
classifier5 = xgboost(data = as.matrix(training_set[-11]), label = training_set$Exited, nrounds = 10)

# Predicting the Test set results
y_pred = predict(classifier5, newdata = as.matrix(test_set[-11]))
y_pred = (y_pred >= 0.5)

# Making the Confusion Matrix
conf_matrix5 = table(test_set[, 11], y_pred)
print(conf_matrix5)
```

### Evalution matrics

```{r}

# Calculate evaluation metrics
accuracy <- sum(diag(conf_matrix5)) / sum(conf_matrix5)
precision <- conf_matrix5[2, 2] / (conf_matrix5[2, 2] + conf_matrix5[1, 2])
recall <- conf_matrix5[2, 2] / sum(conf_matrix5[2, ])
f1_score <- 2 * (precision * recall) / (precision + recall)

# Create a data frame for the metrics
metrics_df <- data.frame(
  Metric = c("Precision", "Recall", "F1 Score", "Accuracy"),
  Value = c(precision, recall, f1_score, accuracy)
)

# Print the metrics table
kable(metrics_df, format = "html", caption = "Evaluation Metrics for Random forest")

```


```{r}
# Applying k-Fold Cross Validation

folds = createFolds(df$Exited, k = 10)
cv = lapply(folds, function(x) {
  training_fold = df[-x, ]
  test_fold = df[x, ]
  classifier = xgboost(data = as.matrix(training_fold[-11]), label = training_fold$Exited, nrounds = 10)
  y_pred = predict(classifier, newdata = as.matrix(test_fold[-11]))
  y_pred = (y_pred >= 0.5)
  cm = table(test_fold[, 11], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
accuracy = mean(as.numeric(cv))
print(accuracy)
```
*Random Forest*

```{r}
training_set$Exited <- factor(training_set$Exited, levels = c(0, 1))
test_set$Exited <- factor(test_set$Exited, levels = c(0, 1))


# Fitting Random Forest Classification to the Training set
set.seed(123)
classifier4 = randomForest(x = training_set[-11],
                          y = training_set$Exited,
                          ntree = 50)

# Predicting the Test set results
y_pred = predict(classifier4, newdata = test_set[-11])
```

```{r}
# Making the Confusion Matrix
conf_matrix4 = table(test_set[, 11], y_pred)
print(conf_matrix4)
```


### Evalution matrics

```{r}

# Calculate evaluation metrics
accuracy <- sum(diag(conf_matrix4)) / sum(conf_matrix4)
precision <- conf_matrix4[2, 2] / (conf_matrix4[2, 2] + conf_matrix4[1, 2])
recall <- conf_matrix4[2, 2] / sum(conf_matrix4[2, ])
f1_score <- 2 * (precision * recall) / (precision + recall)

# Create a data frame for the metrics
metrics_df <- data.frame(
  Metric = c("Precision", "Recall", "F1 Score", "Accuracy"),
  Value = c(precision, recall, f1_score, accuracy)
)

# Print the metrics table
kable(metrics_df, format = "html", caption = "Evaluation Metrics for Random forest")
```



# Conclusion

- After using the models we conclude that *XGBoost* and *RandomForest* are the best model for the problem. 









