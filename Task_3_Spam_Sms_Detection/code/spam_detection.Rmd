---
title: "Spam Sms Detection"
author: "Dipanta"
date: "`r Sys.Date()`"
output: html_document
---

# Problem Statement

We have a dataset which contain information about the Spam and Ham. Spam and Ham Dataset is a collection of text messages that are labeled as either spam or ham (non-spam).Spam messages are usually unsolicited, unwanted, or fraudulent messages that are sent for commercial or malicious purposes. Ham messages are legitimate, personal, or relevant messages that are sent by genuine senders.

Using different type of machine learning model we have to detect tha spam sms.


## Loading Libraries

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(caret)
library(stringr)
library(tm)
library(slam)
library(Matrix)
library(randomForest)
```

## Importing Dataset

```{r}
messages = read.csv("spam.csv")
```


## shape of the dataset

```{r}
head(messages)
```
```{r}
summary(messages)
```
```{r}
colnames(messages)
```

# pre processing
- Drpo unecessary column and rename col 1 and col 2
```{r}

messages = messages %>%
  select(-c("X", "X.1", "X.2")) %>%
  rename(label = v1, text = v2)

head(messages)



```


```{r}

# What portion of our text messages are actually spam?
spam_proportion = prop.table(table(messages$label))["spam"]
cat("Portion of spam messages:", spam_proportion)

```


```{r}
#calculate no of spam and ham

spam_count = sum(messages$label == "spam")
ham_count = sum(messages$label == "ham")

cat("Number of spam messages:", spam_count, "\n")
cat("Number of ham messages:", ham_count, "\n")

```


## Removing Punctuation

```{r}

messages = messages %>%
  mutate(text_clean = str_replace_all(text, "[:punct:]", ""))



```

## Tokenize

```{r}
tokenize = function(text) {
  tokens = unlist(str_split(tolower(text), "\\W+"))
  tokens = tokens[tokens != ""]  # Remove empty tokens
  return(tokens)
}

messages = messages %>%
  mutate(text_tokenized = sapply(text_clean, tokenize))


```

## Remove Stopwords

```{r}
# Load the English stopwords
data("stopwords")
stopwords = stopwords("en")

# Define a function to remove stopwords
remove_stopwords = function(tokenized_text) {
  text = tokenized_text[!(tokenized_text %in% stopwords)]
  return(text)
}

messages = messages %>%
  mutate(text_nostop = lapply(text_tokenized, remove_stopwords))

head(messages)
```


## Applying TF-IDF Vectorizer

```{r warning=FALSE}
# Define a function to clean text
clean_text = function(text) {
  punctuation_chars <- c("!", "\"", "#", "\\$", "%", "&", "'", "\\(", "\\)", "\\*", "\\+", ",", "-", "\\.", "/", ":", ";", "<", "=", ">", "\\?", "@", "\\[", "\\\\", "\\]", "\\^", "_", "`", "\\{", "\\|", "\\}", "~")
  text = tolower(gsub(paste0(punctuation_chars, collapse = "|"), "", text))
  tokens = unlist(str_split(text, "\\W+"))
  tokens = tokens[tokens != ""]
  tokens = tokens[!(tokens %in% stopwords)]
  return(tokens)
}

# Apply the clean_text function
messages$text_clean = lapply(messages$text, clean_text)

# Create a Document-Term Matrix
dtm = DocumentTermMatrix(Corpus(VectorSource(messages$text_clean)))

# Perform TF-IDF transformation
tfidf = weightTfIdf(dtm)

print(dtm)
print(tfidf)
```


```{r}


# Convert sparse matrix to dense matrix (not recommended for large matrices)
dense_matrix = as.matrix(tfidf)

# Convert dense matrix to DataFrame
X_features = as.data.frame(dense_matrix)

# Display the first few rows of the DataFrame
head(X_features)

```
```{r}
# Convert 'label' column to factor with levels "ham" and "spam"
messages$label = factor(messages$label, levels = c("ham", "spam"), labels = c(0, 1))
```

# Model Building

```{r}

# Split the data into training and test sets
set.seed(123)  # For reproducibility
split_index = createDataPartition(messages$label, p = 0.8, list = FALSE)
X_train = X_features[split_index, ]
X_test = X_features[-split_index, ]
y_train = messages$label[split_index]
y_test = messages$label[-split_index]

# Fit a basic Random Forest Model
rf_model = randomForest(x = X_train, y = y_train, ntree= 10)

# Make predictions on the test set using the fitted model
y_pred = predict(rf_model, newdata = X_test)

# Calculate precision and recall using confusionMatrix from the caret package
confusion_matrix = confusionMatrix(data = y_pred, reference = y_test)
precision = confusion_matrix$byClass['Precision']
recall = confusion_matrix$byClass['Recall']

cat('Precision:', round(precision, 3), ' / Recall:', round(recall, 3))

```









